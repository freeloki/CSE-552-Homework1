{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaf database comprises 40 different plant species. Each leaf has 15 different feature including it's class. These features are:\n",
    "\n",
    "1. Class\n",
    "2. Specimen\n",
    "3. Eccentricity\n",
    "4. Aspect Ratio\n",
    "5. Elongation\n",
    "6. Solidity\n",
    "7. Stochastic Convexity\n",
    "8. Isometric Factor\n",
    "9. Maximal Indentation Depth\n",
    "10. Lobedness\n",
    "11. Average Contrast\n",
    "12. Smoothness\n",
    "13. Third Moment\n",
    "14. Uniformity\n",
    "15. Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Specimen</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Aspect Ratio</th>\n",
       "      <th>Elongation</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>Stochastic Convexity</th>\n",
       "      <th>Isometric Factor</th>\n",
       "      <th>Maximal Indentation Depth</th>\n",
       "      <th>Lobedness</th>\n",
       "      <th>Average Contrast</th>\n",
       "      <th>Smoothness</th>\n",
       "      <th>Third Moment</th>\n",
       "      <th>Uniformity</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72694</td>\n",
       "      <td>1.4742</td>\n",
       "      <td>0.32396</td>\n",
       "      <td>0.98535</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.835920</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>0.047790</td>\n",
       "      <td>0.127950</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.74173</td>\n",
       "      <td>1.5257</td>\n",
       "      <td>0.36116</td>\n",
       "      <td>0.98152</td>\n",
       "      <td>0.99825</td>\n",
       "      <td>0.798670</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>0.024160</td>\n",
       "      <td>0.090476</td>\n",
       "      <td>0.008119</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.76722</td>\n",
       "      <td>1.5725</td>\n",
       "      <td>0.38998</td>\n",
       "      <td>0.97755</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.808120</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.011897</td>\n",
       "      <td>0.057445</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.73797</td>\n",
       "      <td>1.4597</td>\n",
       "      <td>0.35376</td>\n",
       "      <td>0.97566</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.816970</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.008607</td>\n",
       "      <td>0.015950</td>\n",
       "      <td>0.065491</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.82301</td>\n",
       "      <td>1.7707</td>\n",
       "      <td>0.44462</td>\n",
       "      <td>0.97698</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.754930</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.010042</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>0.045339</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.72997</td>\n",
       "      <td>1.4892</td>\n",
       "      <td>0.34284</td>\n",
       "      <td>0.98755</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.844820</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>0.010487</td>\n",
       "      <td>0.058528</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.82063</td>\n",
       "      <td>1.7529</td>\n",
       "      <td>0.44458</td>\n",
       "      <td>0.97964</td>\n",
       "      <td>0.99649</td>\n",
       "      <td>0.767700</td>\n",
       "      <td>0.005928</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.018375</td>\n",
       "      <td>0.080587</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.77982</td>\n",
       "      <td>1.6215</td>\n",
       "      <td>0.39222</td>\n",
       "      <td>0.98512</td>\n",
       "      <td>0.99825</td>\n",
       "      <td>0.808160</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.024875</td>\n",
       "      <td>0.089686</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>0.000147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.83089</td>\n",
       "      <td>1.8199</td>\n",
       "      <td>0.45693</td>\n",
       "      <td>0.98240</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.771060</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.007245</td>\n",
       "      <td>0.040616</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.90631</td>\n",
       "      <td>2.3906</td>\n",
       "      <td>0.58336</td>\n",
       "      <td>0.97683</td>\n",
       "      <td>0.99825</td>\n",
       "      <td>0.664190</td>\n",
       "      <td>0.008402</td>\n",
       "      <td>0.012848</td>\n",
       "      <td>0.007010</td>\n",
       "      <td>0.042347</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.74590</td>\n",
       "      <td>1.4927</td>\n",
       "      <td>0.34116</td>\n",
       "      <td>0.98296</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.830880</td>\n",
       "      <td>0.005567</td>\n",
       "      <td>0.005640</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.79606</td>\n",
       "      <td>1.6934</td>\n",
       "      <td>0.43387</td>\n",
       "      <td>0.98181</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.769850</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.013677</td>\n",
       "      <td>0.057832</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93361</td>\n",
       "      <td>2.7582</td>\n",
       "      <td>0.64257</td>\n",
       "      <td>0.98346</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.598510</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.029712</td>\n",
       "      <td>0.089889</td>\n",
       "      <td>0.008015</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.000239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.91186</td>\n",
       "      <td>2.4994</td>\n",
       "      <td>0.60323</td>\n",
       "      <td>0.98300</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.649160</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.072486</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.89063</td>\n",
       "      <td>2.2927</td>\n",
       "      <td>0.56667</td>\n",
       "      <td>0.98732</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.664270</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.029272</td>\n",
       "      <td>0.091328</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.86755</td>\n",
       "      <td>2.0090</td>\n",
       "      <td>0.51464</td>\n",
       "      <td>0.98691</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.702770</td>\n",
       "      <td>0.005444</td>\n",
       "      <td>0.005394</td>\n",
       "      <td>0.030348</td>\n",
       "      <td>0.092063</td>\n",
       "      <td>0.008404</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>0.000199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.91852</td>\n",
       "      <td>2.5247</td>\n",
       "      <td>0.61648</td>\n",
       "      <td>0.97870</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.630370</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>0.023090</td>\n",
       "      <td>0.082029</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.88795</td>\n",
       "      <td>2.2038</td>\n",
       "      <td>0.56218</td>\n",
       "      <td>0.97835</td>\n",
       "      <td>0.99825</td>\n",
       "      <td>0.641580</td>\n",
       "      <td>0.005924</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>0.032722</td>\n",
       "      <td>0.092969</td>\n",
       "      <td>0.008569</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.85121</td>\n",
       "      <td>1.9548</td>\n",
       "      <td>0.48920</td>\n",
       "      <td>0.98622</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.702670</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>0.020258</td>\n",
       "      <td>0.070841</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.89084</td>\n",
       "      <td>2.2979</td>\n",
       "      <td>0.57815</td>\n",
       "      <td>0.97389</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.645980</td>\n",
       "      <td>0.015271</td>\n",
       "      <td>0.042443</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>0.086477</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.93062</td>\n",
       "      <td>2.8973</td>\n",
       "      <td>0.65828</td>\n",
       "      <td>0.98182</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.579500</td>\n",
       "      <td>0.006489</td>\n",
       "      <td>0.007664</td>\n",
       "      <td>0.023606</td>\n",
       "      <td>0.072237</td>\n",
       "      <td>0.005191</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.84113</td>\n",
       "      <td>1.8600</td>\n",
       "      <td>0.46549</td>\n",
       "      <td>0.99039</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.759760</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.132340</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>0.000655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70273</td>\n",
       "      <td>1.2099</td>\n",
       "      <td>0.36317</td>\n",
       "      <td>0.92110</td>\n",
       "      <td>0.98772</td>\n",
       "      <td>0.605550</td>\n",
       "      <td>0.023597</td>\n",
       "      <td>0.101340</td>\n",
       "      <td>0.089301</td>\n",
       "      <td>0.200880</td>\n",
       "      <td>0.038786</td>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66307</td>\n",
       "      <td>1.2065</td>\n",
       "      <td>0.32559</td>\n",
       "      <td>0.94952</td>\n",
       "      <td>0.99649</td>\n",
       "      <td>0.759540</td>\n",
       "      <td>0.013388</td>\n",
       "      <td>0.032621</td>\n",
       "      <td>0.021815</td>\n",
       "      <td>0.097143</td>\n",
       "      <td>0.009348</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.61289</td>\n",
       "      <td>1.0991</td>\n",
       "      <td>0.33117</td>\n",
       "      <td>0.92405</td>\n",
       "      <td>0.98421</td>\n",
       "      <td>0.616610</td>\n",
       "      <td>0.025545</td>\n",
       "      <td>0.118770</td>\n",
       "      <td>0.054687</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.025145</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.70668</td>\n",
       "      <td>1.2510</td>\n",
       "      <td>0.38111</td>\n",
       "      <td>0.94226</td>\n",
       "      <td>0.99825</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.019432</td>\n",
       "      <td>0.068724</td>\n",
       "      <td>0.031587</td>\n",
       "      <td>0.115020</td>\n",
       "      <td>0.013056</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.66889</td>\n",
       "      <td>1.1435</td>\n",
       "      <td>0.38460</td>\n",
       "      <td>0.90355</td>\n",
       "      <td>0.99649</td>\n",
       "      <td>0.605710</td>\n",
       "      <td>0.028329</td>\n",
       "      <td>0.146060</td>\n",
       "      <td>0.057506</td>\n",
       "      <td>0.159310</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.010304</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.50139</td>\n",
       "      <td>1.0066</td>\n",
       "      <td>0.29593</td>\n",
       "      <td>0.91585</td>\n",
       "      <td>0.99825</td>\n",
       "      <td>0.640290</td>\n",
       "      <td>0.021782</td>\n",
       "      <td>0.086347</td>\n",
       "      <td>0.054635</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.024899</td>\n",
       "      <td>0.011106</td>\n",
       "      <td>0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.60803</td>\n",
       "      <td>1.0646</td>\n",
       "      <td>0.34460</td>\n",
       "      <td>0.90487</td>\n",
       "      <td>0.99649</td>\n",
       "      <td>0.675170</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.185380</td>\n",
       "      <td>0.062450</td>\n",
       "      <td>0.164110</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.010602</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.56599</td>\n",
       "      <td>1.0427</td>\n",
       "      <td>0.35318</td>\n",
       "      <td>0.89086</td>\n",
       "      <td>0.99825</td>\n",
       "      <td>0.620680</td>\n",
       "      <td>0.032971</td>\n",
       "      <td>0.197850</td>\n",
       "      <td>0.026348</td>\n",
       "      <td>0.105890</td>\n",
       "      <td>0.011088</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>0.99670</td>\n",
       "      <td>11.3610</td>\n",
       "      <td>0.91400</td>\n",
       "      <td>0.91815</td>\n",
       "      <td>0.93684</td>\n",
       "      <td>0.144350</td>\n",
       "      <td>0.019976</td>\n",
       "      <td>0.072629</td>\n",
       "      <td>0.020667</td>\n",
       "      <td>0.085514</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>0.99841</td>\n",
       "      <td>16.8320</td>\n",
       "      <td>0.94116</td>\n",
       "      <td>0.85531</td>\n",
       "      <td>0.90000</td>\n",
       "      <td>0.094537</td>\n",
       "      <td>0.035845</td>\n",
       "      <td>0.233840</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>0.066682</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>0.99799</td>\n",
       "      <td>15.0680</td>\n",
       "      <td>0.93667</td>\n",
       "      <td>0.88070</td>\n",
       "      <td>0.95789</td>\n",
       "      <td>0.121320</td>\n",
       "      <td>0.032164</td>\n",
       "      <td>0.188280</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>0.054827</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>10.3770</td>\n",
       "      <td>0.90564</td>\n",
       "      <td>0.92135</td>\n",
       "      <td>0.99825</td>\n",
       "      <td>0.179410</td>\n",
       "      <td>0.016647</td>\n",
       "      <td>0.050433</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.071662</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.000170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>0.99505</td>\n",
       "      <td>10.7360</td>\n",
       "      <td>0.90851</td>\n",
       "      <td>0.92586</td>\n",
       "      <td>0.99649</td>\n",
       "      <td>0.182360</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.148560</td>\n",
       "      <td>0.017761</td>\n",
       "      <td>0.069610</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>0.99518</td>\n",
       "      <td>10.4210</td>\n",
       "      <td>0.90385</td>\n",
       "      <td>0.96551</td>\n",
       "      <td>0.98947</td>\n",
       "      <td>0.200510</td>\n",
       "      <td>0.007723</td>\n",
       "      <td>0.010855</td>\n",
       "      <td>0.021258</td>\n",
       "      <td>0.080385</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>0.99530</td>\n",
       "      <td>10.6300</td>\n",
       "      <td>0.90598</td>\n",
       "      <td>0.88866</td>\n",
       "      <td>0.95789</td>\n",
       "      <td>0.159670</td>\n",
       "      <td>0.025636</td>\n",
       "      <td>0.119610</td>\n",
       "      <td>0.014782</td>\n",
       "      <td>0.065416</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.99515</td>\n",
       "      <td>10.0070</td>\n",
       "      <td>0.90328</td>\n",
       "      <td>0.91594</td>\n",
       "      <td>0.97719</td>\n",
       "      <td>0.162440</td>\n",
       "      <td>0.033129</td>\n",
       "      <td>0.199750</td>\n",
       "      <td>0.017258</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>0.99871</td>\n",
       "      <td>19.0380</td>\n",
       "      <td>0.94834</td>\n",
       "      <td>0.85100</td>\n",
       "      <td>0.90702</td>\n",
       "      <td>0.086183</td>\n",
       "      <td>0.073048</td>\n",
       "      <td>0.971170</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.048089</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91090</td>\n",
       "      <td>2.5488</td>\n",
       "      <td>0.61060</td>\n",
       "      <td>0.97388</td>\n",
       "      <td>0.99825</td>\n",
       "      <td>0.558180</td>\n",
       "      <td>0.019720</td>\n",
       "      <td>0.070775</td>\n",
       "      <td>0.101320</td>\n",
       "      <td>0.170220</td>\n",
       "      <td>0.028158</td>\n",
       "      <td>0.007461</td>\n",
       "      <td>0.001097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.90391</td>\n",
       "      <td>2.4580</td>\n",
       "      <td>0.59770</td>\n",
       "      <td>0.98610</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.614290</td>\n",
       "      <td>0.006701</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>0.089020</td>\n",
       "      <td>0.167060</td>\n",
       "      <td>0.027151</td>\n",
       "      <td>0.007219</td>\n",
       "      <td>0.000966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90755</td>\n",
       "      <td>2.5820</td>\n",
       "      <td>0.62394</td>\n",
       "      <td>0.96837</td>\n",
       "      <td>0.99825</td>\n",
       "      <td>0.556740</td>\n",
       "      <td>0.031714</td>\n",
       "      <td>0.183050</td>\n",
       "      <td>0.079387</td>\n",
       "      <td>0.162130</td>\n",
       "      <td>0.025613</td>\n",
       "      <td>0.007412</td>\n",
       "      <td>0.000699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>0.91708</td>\n",
       "      <td>2.6498</td>\n",
       "      <td>0.62919</td>\n",
       "      <td>0.98493</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.579260</td>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>0.082187</td>\n",
       "      <td>0.168770</td>\n",
       "      <td>0.027693</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>0.91660</td>\n",
       "      <td>2.6711</td>\n",
       "      <td>0.63330</td>\n",
       "      <td>0.98228</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.516190</td>\n",
       "      <td>0.017465</td>\n",
       "      <td>0.055514</td>\n",
       "      <td>0.103680</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>0.008198</td>\n",
       "      <td>0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>0.89420</td>\n",
       "      <td>2.4203</td>\n",
       "      <td>0.59323</td>\n",
       "      <td>0.98407</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.607940</td>\n",
       "      <td>0.017430</td>\n",
       "      <td>0.055294</td>\n",
       "      <td>0.101020</td>\n",
       "      <td>0.187480</td>\n",
       "      <td>0.033957</td>\n",
       "      <td>0.009815</td>\n",
       "      <td>0.001179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.93847</td>\n",
       "      <td>3.0198</td>\n",
       "      <td>0.67312</td>\n",
       "      <td>0.98733</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.532340</td>\n",
       "      <td>0.007639</td>\n",
       "      <td>0.010621</td>\n",
       "      <td>0.076290</td>\n",
       "      <td>0.180170</td>\n",
       "      <td>0.031440</td>\n",
       "      <td>0.011932</td>\n",
       "      <td>0.000723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>0.89518</td>\n",
       "      <td>2.3875</td>\n",
       "      <td>0.58100</td>\n",
       "      <td>0.98796</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.638330</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>0.011052</td>\n",
       "      <td>0.073040</td>\n",
       "      <td>0.159100</td>\n",
       "      <td>0.024688</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.000841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>0.91707</td>\n",
       "      <td>2.6504</td>\n",
       "      <td>0.63359</td>\n",
       "      <td>0.96002</td>\n",
       "      <td>0.99298</td>\n",
       "      <td>0.539720</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.026481</td>\n",
       "      <td>0.101210</td>\n",
       "      <td>0.184330</td>\n",
       "      <td>0.032861</td>\n",
       "      <td>0.008966</td>\n",
       "      <td>0.001415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>0.92535</td>\n",
       "      <td>2.8030</td>\n",
       "      <td>0.65133</td>\n",
       "      <td>0.97600</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.484130</td>\n",
       "      <td>0.015435</td>\n",
       "      <td>0.043360</td>\n",
       "      <td>0.098946</td>\n",
       "      <td>0.183380</td>\n",
       "      <td>0.032533</td>\n",
       "      <td>0.009180</td>\n",
       "      <td>0.001330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>0.91861</td>\n",
       "      <td>2.8114</td>\n",
       "      <td>0.64707</td>\n",
       "      <td>0.94843</td>\n",
       "      <td>0.95614</td>\n",
       "      <td>0.511860</td>\n",
       "      <td>0.045037</td>\n",
       "      <td>0.369150</td>\n",
       "      <td>0.041345</td>\n",
       "      <td>0.132970</td>\n",
       "      <td>0.017374</td>\n",
       "      <td>0.007023</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0.39093</td>\n",
       "      <td>1.1025</td>\n",
       "      <td>0.73351</td>\n",
       "      <td>0.72022</td>\n",
       "      <td>0.69474</td>\n",
       "      <td>0.179540</td>\n",
       "      <td>0.076072</td>\n",
       "      <td>1.053200</td>\n",
       "      <td>0.059213</td>\n",
       "      <td>0.157470</td>\n",
       "      <td>0.024197</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.47124</td>\n",
       "      <td>1.1349</td>\n",
       "      <td>0.81159</td>\n",
       "      <td>0.65915</td>\n",
       "      <td>0.47368</td>\n",
       "      <td>0.093982</td>\n",
       "      <td>0.096492</td>\n",
       "      <td>1.694500</td>\n",
       "      <td>0.098618</td>\n",
       "      <td>0.210620</td>\n",
       "      <td>0.042478</td>\n",
       "      <td>0.016848</td>\n",
       "      <td>0.000581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.36870</td>\n",
       "      <td>1.0456</td>\n",
       "      <td>0.77124</td>\n",
       "      <td>0.74413</td>\n",
       "      <td>0.77368</td>\n",
       "      <td>0.222780</td>\n",
       "      <td>0.075187</td>\n",
       "      <td>1.028900</td>\n",
       "      <td>0.074488</td>\n",
       "      <td>0.179930</td>\n",
       "      <td>0.031359</td>\n",
       "      <td>0.012414</td>\n",
       "      <td>0.000347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>0.14986</td>\n",
       "      <td>1.0558</td>\n",
       "      <td>0.77733</td>\n",
       "      <td>0.73454</td>\n",
       "      <td>0.66316</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>0.080410</td>\n",
       "      <td>1.176800</td>\n",
       "      <td>0.103950</td>\n",
       "      <td>0.180600</td>\n",
       "      <td>0.031585</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.001432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>0.68069</td>\n",
       "      <td>1.1866</td>\n",
       "      <td>0.78745</td>\n",
       "      <td>0.73496</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>0.140290</td>\n",
       "      <td>0.072447</td>\n",
       "      <td>0.955240</td>\n",
       "      <td>0.092770</td>\n",
       "      <td>0.184510</td>\n",
       "      <td>0.032923</td>\n",
       "      <td>0.010852</td>\n",
       "      <td>0.000594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>0.37522</td>\n",
       "      <td>1.1417</td>\n",
       "      <td>0.81725</td>\n",
       "      <td>0.68511</td>\n",
       "      <td>0.58772</td>\n",
       "      <td>0.125230</td>\n",
       "      <td>0.091860</td>\n",
       "      <td>1.535800</td>\n",
       "      <td>0.114880</td>\n",
       "      <td>0.208610</td>\n",
       "      <td>0.041703</td>\n",
       "      <td>0.013344</td>\n",
       "      <td>0.000820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.28064</td>\n",
       "      <td>1.0849</td>\n",
       "      <td>0.75319</td>\n",
       "      <td>0.72152</td>\n",
       "      <td>0.71404</td>\n",
       "      <td>0.136860</td>\n",
       "      <td>0.078996</td>\n",
       "      <td>1.135800</td>\n",
       "      <td>0.141220</td>\n",
       "      <td>0.218300</td>\n",
       "      <td>0.045488</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>0.001515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>0.35344</td>\n",
       "      <td>1.0329</td>\n",
       "      <td>0.78147</td>\n",
       "      <td>0.70737</td>\n",
       "      <td>0.61579</td>\n",
       "      <td>0.135030</td>\n",
       "      <td>0.089763</td>\n",
       "      <td>1.466400</td>\n",
       "      <td>0.097663</td>\n",
       "      <td>0.207030</td>\n",
       "      <td>0.041101</td>\n",
       "      <td>0.016123</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>0.59988</td>\n",
       "      <td>1.1427</td>\n",
       "      <td>0.71532</td>\n",
       "      <td>0.66101</td>\n",
       "      <td>0.47544</td>\n",
       "      <td>0.157470</td>\n",
       "      <td>0.113370</td>\n",
       "      <td>2.339400</td>\n",
       "      <td>0.050389</td>\n",
       "      <td>0.135850</td>\n",
       "      <td>0.018121</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>0.47195</td>\n",
       "      <td>1.0901</td>\n",
       "      <td>0.85409</td>\n",
       "      <td>0.53598</td>\n",
       "      <td>0.39649</td>\n",
       "      <td>0.078376</td>\n",
       "      <td>0.132270</td>\n",
       "      <td>3.184000</td>\n",
       "      <td>0.082007</td>\n",
       "      <td>0.187820</td>\n",
       "      <td>0.034074</td>\n",
       "      <td>0.013487</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class  Specimen  Eccentricity  Aspect Ratio  Elongation  Solidity  \\\n",
       "0        1         1       0.72694        1.4742     0.32396   0.98535   \n",
       "1        1         2       0.74173        1.5257     0.36116   0.98152   \n",
       "2        1         3       0.76722        1.5725     0.38998   0.97755   \n",
       "3        1         4       0.73797        1.4597     0.35376   0.97566   \n",
       "4        1         5       0.82301        1.7707     0.44462   0.97698   \n",
       "5        1         6       0.72997        1.4892     0.34284   0.98755   \n",
       "6        1         7       0.82063        1.7529     0.44458   0.97964   \n",
       "7        1         8       0.77982        1.6215     0.39222   0.98512   \n",
       "8        1         9       0.83089        1.8199     0.45693   0.98240   \n",
       "9        1        10       0.90631        2.3906     0.58336   0.97683   \n",
       "10       1        11       0.74590        1.4927     0.34116   0.98296   \n",
       "11       1        12       0.79606        1.6934     0.43387   0.98181   \n",
       "12       2         1       0.93361        2.7582     0.64257   0.98346   \n",
       "13       2         2       0.91186        2.4994     0.60323   0.98300   \n",
       "14       2         3       0.89063        2.2927     0.56667   0.98732   \n",
       "15       2         4       0.86755        2.0090     0.51464   0.98691   \n",
       "16       2         5       0.91852        2.5247     0.61648   0.97870   \n",
       "17       2         6       0.88795        2.2038     0.56218   0.97835   \n",
       "18       2         7       0.85121        1.9548     0.48920   0.98622   \n",
       "19       2         8       0.89084        2.2979     0.57815   0.97389   \n",
       "20       2         9       0.93062        2.8973     0.65828   0.98182   \n",
       "21       2        10       0.84113        1.8600     0.46549   0.99039   \n",
       "22       3         1       0.70273        1.2099     0.36317   0.92110   \n",
       "23       3         2       0.66307        1.2065     0.32559   0.94952   \n",
       "24       3         3       0.61289        1.0991     0.33117   0.92405   \n",
       "25       3         4       0.70668        1.2510     0.38111   0.94226   \n",
       "26       3         5       0.66889        1.1435     0.38460   0.90355   \n",
       "27       3         6       0.50139        1.0066     0.29593   0.91585   \n",
       "28       3         7       0.60803        1.0646     0.34460   0.90487   \n",
       "29       3         8       0.56599        1.0427     0.35318   0.89086   \n",
       "..     ...       ...           ...           ...         ...       ...   \n",
       "310     34         3       0.99670       11.3610     0.91400   0.91815   \n",
       "311     34         4       0.99841       16.8320     0.94116   0.85531   \n",
       "312     34         5       0.99799       15.0680     0.93667   0.88070   \n",
       "313     34         6       0.99512       10.3770     0.90564   0.92135   \n",
       "314     34         7       0.99505       10.7360     0.90851   0.92586   \n",
       "315     34         8       0.99518       10.4210     0.90385   0.96551   \n",
       "316     34         9       0.99530       10.6300     0.90598   0.88866   \n",
       "317     34        10       0.99515       10.0070     0.90328   0.91594   \n",
       "318     34        11       0.99871       19.0380     0.94834   0.85100   \n",
       "319     35         1       0.91090        2.5488     0.61060   0.97388   \n",
       "320     35         2       0.90391        2.4580     0.59770   0.98610   \n",
       "321     35         3       0.90755        2.5820     0.62394   0.96837   \n",
       "322     35         4       0.91708        2.6498     0.62919   0.98493   \n",
       "323     35         5       0.91660        2.6711     0.63330   0.98228   \n",
       "324     35         6       0.89420        2.4203     0.59323   0.98407   \n",
       "325     35         7       0.93847        3.0198     0.67312   0.98733   \n",
       "326     35         8       0.89518        2.3875     0.58100   0.98796   \n",
       "327     35         9       0.91707        2.6504     0.63359   0.96002   \n",
       "328     35        10       0.92535        2.8030     0.65133   0.97600   \n",
       "329     35        11       0.91861        2.8114     0.64707   0.94843   \n",
       "330     36         1       0.39093        1.1025     0.73351   0.72022   \n",
       "331     36         2       0.47124        1.1349     0.81159   0.65915   \n",
       "332     36         3       0.36870        1.0456     0.77124   0.74413   \n",
       "333     36         4       0.14986        1.0558     0.77733   0.73454   \n",
       "334     36         5       0.68069        1.1866     0.78745   0.73496   \n",
       "335     36         6       0.37522        1.1417     0.81725   0.68511   \n",
       "336     36         7       0.28064        1.0849     0.75319   0.72152   \n",
       "337     36         8       0.35344        1.0329     0.78147   0.70737   \n",
       "338     36         9       0.59988        1.1427     0.71532   0.66101   \n",
       "339     36        10       0.47195        1.0901     0.85409   0.53598   \n",
       "\n",
       "     Stochastic Convexity  Isometric Factor  Maximal Indentation Depth  \\\n",
       "0                 1.00000          0.835920                   0.004657   \n",
       "1                 0.99825          0.798670                   0.005242   \n",
       "2                 1.00000          0.808120                   0.007457   \n",
       "3                 1.00000          0.816970                   0.006877   \n",
       "4                 1.00000          0.754930                   0.007428   \n",
       "5                 1.00000          0.844820                   0.004945   \n",
       "6                 0.99649          0.767700                   0.005928   \n",
       "7                 0.99825          0.808160                   0.005099   \n",
       "8                 1.00000          0.771060                   0.006005   \n",
       "9                 0.99825          0.664190                   0.008402   \n",
       "10                1.00000          0.830880                   0.005567   \n",
       "11                1.00000          0.769850                   0.007799   \n",
       "12                1.00000          0.598510                   0.005534   \n",
       "13                1.00000          0.649160                   0.006149   \n",
       "14                1.00000          0.664270                   0.002837   \n",
       "15                1.00000          0.702770                   0.005444   \n",
       "16                1.00000          0.630370                   0.005049   \n",
       "17                0.99825          0.641580                   0.005924   \n",
       "18                1.00000          0.702670                   0.003973   \n",
       "19                1.00000          0.645980                   0.015271   \n",
       "20                1.00000          0.579500                   0.006489   \n",
       "21                1.00000          0.759760                   0.004676   \n",
       "22                0.98772          0.605550                   0.023597   \n",
       "23                0.99649          0.759540                   0.013388   \n",
       "24                0.98421          0.616610                   0.025545   \n",
       "25                0.99825          0.692500                   0.019432   \n",
       "26                0.99649          0.605710                   0.028329   \n",
       "27                0.99825          0.640290                   0.021782   \n",
       "28                0.99649          0.675170                   0.031915   \n",
       "29                0.99825          0.620680                   0.032971   \n",
       "..                    ...               ...                        ...   \n",
       "310               0.93684          0.144350                   0.019976   \n",
       "311               0.90000          0.094537                   0.035845   \n",
       "312               0.95789          0.121320                   0.032164   \n",
       "313               0.99825          0.179410                   0.016647   \n",
       "314               0.99649          0.182360                   0.028571   \n",
       "315               0.98947          0.200510                   0.007723   \n",
       "316               0.95789          0.159670                   0.025636   \n",
       "317               0.97719          0.162440                   0.033129   \n",
       "318               0.90702          0.086183                   0.073048   \n",
       "319               0.99825          0.558180                   0.019720   \n",
       "320               1.00000          0.614290                   0.006701   \n",
       "321               0.99825          0.556740                   0.031714   \n",
       "322               1.00000          0.579260                   0.006805   \n",
       "323               1.00000          0.516190                   0.017465   \n",
       "324               1.00000          0.607940                   0.017430   \n",
       "325               1.00000          0.532340                   0.007639   \n",
       "326               1.00000          0.638330                   0.007793   \n",
       "327               0.99298          0.539720                   0.012062   \n",
       "328               1.00000          0.484130                   0.015435   \n",
       "329               0.95614          0.511860                   0.045037   \n",
       "330               0.69474          0.179540                   0.076072   \n",
       "331               0.47368          0.093982                   0.096492   \n",
       "332               0.77368          0.222780                   0.075187   \n",
       "333               0.66316          0.168800                   0.080410   \n",
       "334               0.60000          0.140290                   0.072447   \n",
       "335               0.58772          0.125230                   0.091860   \n",
       "336               0.71404          0.136860                   0.078996   \n",
       "337               0.61579          0.135030                   0.089763   \n",
       "338               0.47544          0.157470                   0.113370   \n",
       "339               0.39649          0.078376                   0.132270   \n",
       "\n",
       "     Lobedness  Average Contrast  Smoothness  Third Moment  Uniformity  \\\n",
       "0     0.003947          0.047790    0.127950      0.016108    0.005232   \n",
       "1     0.005002          0.024160    0.090476      0.008119    0.002708   \n",
       "2     0.010121          0.011897    0.057445      0.003289    0.000921   \n",
       "3     0.008607          0.015950    0.065491      0.004271    0.001154   \n",
       "4     0.010042          0.007938    0.045339      0.002051    0.000560   \n",
       "5     0.004451          0.010487    0.058528      0.003414    0.001125   \n",
       "6     0.006395          0.018375    0.080587      0.006452    0.002271   \n",
       "7     0.004731          0.024875    0.089686      0.007979    0.002466   \n",
       "8     0.006564          0.007245    0.040616      0.001647    0.000388   \n",
       "9     0.012848          0.007010    0.042347      0.001790    0.000459   \n",
       "10    0.005640          0.005768    0.036511      0.001331    0.000309   \n",
       "11    0.011071          0.013677    0.057832      0.003333    0.000816   \n",
       "12    0.005573          0.029712    0.089889      0.008015    0.002065   \n",
       "13    0.006882          0.018887    0.072486      0.005227    0.001489   \n",
       "14    0.001464          0.029272    0.091328      0.008272    0.002238   \n",
       "15    0.005394          0.030348    0.092063      0.008404    0.002254   \n",
       "16    0.004640          0.023090    0.082029      0.006684    0.001893   \n",
       "17    0.006387          0.032722    0.092969      0.008569    0.002120   \n",
       "18    0.002873          0.020258    0.070841      0.004993    0.001227   \n",
       "19    0.042443          0.028461    0.086477      0.007423    0.001883   \n",
       "20    0.007664          0.023606    0.072237      0.005191    0.001122   \n",
       "21    0.003979          0.062798    0.132340      0.017213    0.004453   \n",
       "22    0.101340          0.089301    0.200880      0.038786    0.015895   \n",
       "23    0.032621          0.021815    0.097143      0.009348    0.004028   \n",
       "24    0.118770          0.054687    0.160600      0.025145    0.011672   \n",
       "25    0.068724          0.031587    0.115020      0.013056    0.005311   \n",
       "26    0.146060          0.057506    0.159310      0.024752    0.010304   \n",
       "27    0.086347          0.054635    0.159800      0.024899    0.011106   \n",
       "28    0.185380          0.062450    0.164110      0.026225    0.010602   \n",
       "29    0.197850          0.026348    0.105890      0.011088    0.004651   \n",
       "..         ...               ...         ...           ...         ...   \n",
       "310   0.072629          0.020667    0.085514      0.007259    0.002847   \n",
       "311   0.233840          0.012341    0.066682      0.004427    0.001705   \n",
       "312   0.188280          0.011686    0.054827      0.002997    0.000864   \n",
       "313   0.050433          0.020400    0.071662      0.005109    0.001266   \n",
       "314   0.148560          0.017761    0.069610      0.004822    0.001313   \n",
       "315   0.010855          0.021258    0.080385      0.006420    0.001917   \n",
       "316   0.119610          0.014782    0.065416      0.004261    0.001283   \n",
       "317   0.199750          0.017258    0.070646      0.004966    0.001464   \n",
       "318   0.971170          0.007817    0.048089      0.002307    0.000753   \n",
       "319   0.070775          0.101320    0.170220      0.028158    0.007461   \n",
       "320   0.008173          0.089020    0.167060      0.027151    0.007219   \n",
       "321   0.183050          0.079387    0.162130      0.025613    0.007412   \n",
       "322   0.008429          0.082187    0.168770      0.027693    0.008503   \n",
       "323   0.055514          0.103680    0.181000      0.031721    0.008198   \n",
       "324   0.055294          0.101020    0.187480      0.033957    0.009815   \n",
       "325   0.010621          0.076290    0.180170      0.031440    0.011932   \n",
       "326   0.011052          0.073040    0.159100      0.024688    0.007381   \n",
       "327   0.026481          0.101210    0.184330      0.032861    0.008966   \n",
       "328   0.043360          0.098946    0.183380      0.032533    0.009180   \n",
       "329   0.369150          0.041345    0.132970      0.017374    0.007023   \n",
       "330   1.053200          0.059213    0.157470      0.024197    0.009541   \n",
       "331   1.694500          0.098618    0.210620      0.042478    0.016848   \n",
       "332   1.028900          0.074488    0.179930      0.031359    0.012414   \n",
       "333   1.176800          0.103950    0.180600      0.031585    0.008017   \n",
       "334   0.955240          0.092770    0.184510      0.032923    0.010852   \n",
       "335   1.535800          0.114880    0.208610      0.041703    0.013344   \n",
       "336   1.135800          0.141220    0.218300      0.045488    0.012002   \n",
       "337   1.466400          0.097663    0.207030      0.041101    0.016123   \n",
       "338   2.339400          0.050389    0.135850      0.018121    0.006190   \n",
       "339   3.184000          0.082007    0.187820      0.034074    0.013487   \n",
       "\n",
       "      Entropy  \n",
       "0    0.000275  \n",
       "1    0.000075  \n",
       "2    0.000038  \n",
       "3    0.000066  \n",
       "4    0.000024  \n",
       "5    0.000025  \n",
       "6    0.000041  \n",
       "7    0.000147  \n",
       "8    0.000033  \n",
       "9    0.000028  \n",
       "10   0.000032  \n",
       "11   0.000139  \n",
       "12   0.000239  \n",
       "13   0.000083  \n",
       "14   0.000202  \n",
       "15   0.000199  \n",
       "16   0.000125  \n",
       "17   0.000277  \n",
       "18   0.000149  \n",
       "19   0.000243  \n",
       "20   0.000256  \n",
       "21   0.000655  \n",
       "22   0.000405  \n",
       "23   0.000036  \n",
       "24   0.000121  \n",
       "25   0.000086  \n",
       "26   0.000185  \n",
       "27   0.000162  \n",
       "28   0.000230  \n",
       "29   0.000052  \n",
       "..        ...  \n",
       "310  0.000046  \n",
       "311  0.000018  \n",
       "312  0.000044  \n",
       "313  0.000170  \n",
       "314  0.000131  \n",
       "315  0.000096  \n",
       "316  0.000064  \n",
       "317  0.000072  \n",
       "318  0.000013  \n",
       "319  0.001097  \n",
       "320  0.000966  \n",
       "321  0.000699  \n",
       "322  0.000640  \n",
       "323  0.001421  \n",
       "324  0.001179  \n",
       "325  0.000723  \n",
       "326  0.000841  \n",
       "327  0.001415  \n",
       "328  0.001330  \n",
       "329  0.000189  \n",
       "330  0.000247  \n",
       "331  0.000581  \n",
       "332  0.000347  \n",
       "333  0.001432  \n",
       "334  0.000594  \n",
       "335  0.000820  \n",
       "336  0.001515  \n",
       "337  0.000453  \n",
       "338  0.000265  \n",
       "339  0.000329  \n",
       "\n",
       "[340 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils.data_utils as data_util\n",
    "\n",
    "\n",
    "data = data_util.load_data()\n",
    "\n",
    "#print data\n",
    "data.head(340)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training and Testing Data\n",
    "I will use 80% of the data as training data. I will use the rest of the data is for testing. After splitting data I have:\n",
    "- 257 data for training.\n",
    "- 83 data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  257\n",
      "Test:  83\n",
      "(257, 15)\n",
      "(83, 15)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Splitting Training and Testing Data\n",
    "\n",
    "training_data, testing_data = data_util.split_training_and_testing_data(data)\n",
    "\n",
    "total_train = 0\n",
    "for x in range(0,len(training_data)):\n",
    "    total_train += len(training_data[x])\n",
    "\n",
    "print(\"Train: \", total_train )\n",
    "total_test = 0\n",
    "for x in range(0,len(testing_data)):\n",
    "    total_test += len(testing_data[x])\n",
    "print(\"Test: \", total_test )\n",
    "\n",
    "train_numpy = np.asarray(training_data[0])\n",
    "test_numpy = np.asarray(testing_data[0])\n",
    "for x in range(1,len(training_data)):\n",
    "    train_numpy = np.concatenate((train_numpy,np.asarray(training_data[x])), axis=0 )\n",
    "    \n",
    "for x in range(1,len(testing_data)):\n",
    "    test_numpy = np.concatenate((test_numpy,np.asarray(testing_data[x])), axis=0 )\n",
    "\n",
    "\n",
    "#print(test_numpy)\n",
    "print(train_numpy.shape)\n",
    "print(test_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.0\n",
      "[220]\n"
     ]
    }
   ],
   "source": [
    "import utils.data_utils as data_util\n",
    "\n",
    "sorted, neigbours = data_util.k_nearest_neighbourhood(train_numpy,test_numpy[0],1)\n",
    "\n",
    "print(sorted)\n",
    "print(neigbours)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 KNN with Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Besk K Parameter \n",
    "I will make a greedy search for finding best k value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n",
      "81\n",
      "Splits: 5\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 200 TestSize:49\n",
      "K SIZE : 1 MEAN ACC  % 61.46122448979592\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 200 TestSize:49\n",
      "K SIZE : 3 MEAN ACC  % 53.01224489795918\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 200 TestSize:49\n",
      "K SIZE : 5 MEAN ACC  % 52.220408163265304\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 200 TestSize:49\n",
      "K SIZE : 7 MEAN ACC  % 48.99591836734694\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 200 TestSize:49\n",
      "K SIZE : 9 MEAN ACC  % 44.587755102040816\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 200 TestSize:49\n",
      "K SIZE : 11 MEAN ACC  % 43.78775510204082\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 199 TestSize:50\n",
      "TrainSize: 200 TestSize:49\n",
      "K SIZE : 13 MEAN ACC  % 42.17142857142857\n",
      "{1: 61.46122448979592, 3: 53.01224489795918, 5: 52.220408163265304, 7: 48.99591836734694, 9: 44.587755102040816, 11: 43.78775510204082, 13: 42.17142857142857}\n",
      "Test1\n",
      "Accuracy Rate % 56.79012345679013\n",
      "Train1\n",
      "Accuracy Rate % 100.0\n",
      "----------------------\n",
      "\n",
      "Test3\n",
      "Accuracy Rate % 51.851851851851855\n",
      "Train3\n",
      "Accuracy Rate % 78.714859437751\n",
      "----------------------\n",
      "\n",
      "Test5\n",
      "Accuracy Rate % 55.55555555555556\n",
      "Train5\n",
      "Accuracy Rate % 75.1004016064257\n",
      "----------------------\n",
      "\n",
      "Test7\n",
      "Accuracy Rate % 54.32098765432099\n",
      "Train7\n",
      "Accuracy Rate % 69.07630522088354\n",
      "----------------------\n",
      "\n",
      "Test9\n",
      "Accuracy Rate % 50.617283950617285\n",
      "Train9\n",
      "Accuracy Rate % 65.86345381526104\n",
      "----------------------\n",
      "\n",
      "Test11\n",
      "Accuracy Rate % 50.617283950617285\n",
      "Train11\n",
      "Accuracy Rate % 61.84738955823293\n",
      "----------------------\n",
      "\n",
      "Test13\n",
      "Accuracy Rate % 50.617283950617285\n",
      "Train13\n",
      "Accuracy Rate % 61.04417670682731\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "training_data, testing_data = data_util.split_training_and_testing_data(data)\n",
    "\n",
    "train_numpy = np.asarray(training_data[0])\n",
    "test_numpy = np.asarray(testing_data[0])\n",
    "# make numpy array here.# start from 2 we dont need them.\n",
    "for x in range(2, len(training_data)):\n",
    "    train_numpy = np.concatenate((train_numpy, np.asarray(training_data[x])), axis=0)\n",
    "\n",
    "for x in range(2, len(testing_data)):\n",
    "    test_numpy = np.concatenate((test_numpy, np.asarray(testing_data[x])), axis=0)\n",
    "\n",
    "print(len(train_numpy))\n",
    "print(len(test_numpy))\n",
    "def find_best_k_value(train_numpy):\n",
    "    train_results = {}\n",
    "    split_size = 5\n",
    "    kf = KFold(n_splits=split_size, random_state=False, shuffle=True)\n",
    "\n",
    "    splits = kf.get_n_splits(train_numpy)\n",
    "\n",
    "    print(\"Splits: \" + str(splits))\n",
    "\n",
    "    for k_size in range(1, 15, 2):\n",
    "        mean_pos = 0\n",
    "        temp_acc = 0\n",
    "       \n",
    "        for train_index, test_index in kf.split(train_numpy):\n",
    "            # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train = []\n",
    "            X_test = []\n",
    "            X_train, X_test = train_numpy[train_index], train_numpy[test_index]\n",
    "\n",
    "            positive = 0\n",
    "            negative = 0\n",
    "            for x in range(0, len(X_test)):\n",
    "                sorted2, neigbours2 = data_util.k_nearest_neighbourhood(X_train, X_test[x], k_size)\n",
    "                if int(X_test[x][0]) == int(sorted2):\n",
    "                    positive += 1\n",
    "                else:\n",
    "                    negative += 1\n",
    "\n",
    "            temp_acc = (100 * positive) / (positive + negative)\n",
    "            mean_pos += temp_acc\n",
    "            print(\"TrainSize: \" + str(len(X_train)) + \" TestSize:\" + str(len(X_test)))\n",
    "        # print(\"K : \" + str(k_size) + \" True:\" + str(positive) + \" False: \" + str(negative) + \"  Positive Accuracy Rate % \" + str(\n",
    "        # (100 * positive) / (positive + negative)))\n",
    "        train_results[k_size] = mean_pos / split_size\n",
    "        # print(\"K : \" + str(k_size) + \" True:\" + str(positive) + \" False: \" + str(negative) + \"  Negative Accuracy Rate % \" + str(\n",
    "        # (100 * negative) / (positive + negative)))\n",
    "\n",
    "        print(\"K SIZE : \" + str(k_size) + \" MEAN ACC  % \" + str(mean_pos / split_size))\n",
    "    return train_results\n",
    "\n",
    "## DATA is NOT NORMALIZED\n",
    "print(find_best_k_value(train_numpy))\n",
    "\n",
    "def calcTestScore(X_train, X_test,k_size):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for x in range(0, len(X_test)):\n",
    "        sorted2, neigbours2 = data_util.k_nearest_neighbourhood(X_train, X_test[x],k_size)\n",
    "        if int(X_test[x][0]) == int(sorted2):\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    print( \"Accuracy Rate % \" + str((100 * positive) / (positive + negative)))\n",
    "\n",
    "for index in range(1,15,2):\n",
    "    print(\"Test\"+ str(index))\n",
    "    calcTestScore(train_numpy, test_numpy,index)\n",
    "    print(\"Train\"+ str(index))\n",
    "    calcTestScore(train_numpy, train_numpy,index)\n",
    "    print(\"----------------------\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without normalizing data we the best accuracy at K=1 and accuracy for TRAIN % 61.46122448979592, TEST: %56.\n",
    "\n",
    "Now lets try to use the same approach with normalized data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 2.00000000e+00 1.33818352e-01 2.75257384e-01\n",
      " 6.51582597e-02 1.77079785e-01 1.80098108e-01 1.44091116e-01\n",
      " 9.45783434e-04 9.02357825e-04 4.35879819e-03 1.63231219e-02\n",
      " 1.46487011e-03 4.88560658e-04 1.35032537e-05]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'find_best_k_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5c759bef00a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_best_k_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalcTestScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_best_k_value' is not defined"
     ]
    }
   ],
   "source": [
    "## Normalized Data\n",
    "def normalizeData(norm_data):\n",
    "    for x in range(0, len(norm_data), 1):\n",
    "        temp_point = norm_data[x]\n",
    "        # print(\"BEFORE_NORM\")\n",
    "        # print(temp_point)\n",
    "        total_sum = sum(temp_point) - norm_data[x][0] - norm_data[x][1]\n",
    "        for y in range(2, len(temp_point)):\n",
    "            temp_point[y] = temp_point[y] / total_sum\n",
    "\n",
    "        # print(\"AFTER_NORM\")\n",
    "        # print(temp_point)\n",
    "        norm_data[x] = temp_point\n",
    "    \n",
    "    return norm_data\n",
    "\n",
    "\n",
    "norm_train = normalizeData(train_numpy)\n",
    "norm_test = normalizeData(test_numpy)\n",
    "\n",
    "print(norm_train[1])\n",
    "\n",
    "print(find_best_k_value(norm_train))\n",
    "\n",
    "def calcTestScore(X_train, X_test,k_size):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for x in range(0, len(X_test)):\n",
    "        sorted2, neigbours2 = data_util.k_nearest_neighbourhood(X_train, X_test[x],k_size)\n",
    "        if int(X_test[x][0]) == int(sorted2):\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    print( \"Accuracy Rate % \" + str((100 * positive) / (positive + negative)))\n",
    "\n",
    "for index in range(1,15,2):\n",
    "    print(\"Test\"+ str(index))\n",
    "    calcTestScore(norm_train, norm_test,index)\n",
    "    print(\"Train\"+ str(index))\n",
    "    calcTestScore(norm_train, norm_train,index)\n",
    "    print(\"----------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalizing data increased our accuracy. Train: %88 Test:%60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same approach we sklearn's KNN. I have added this code the benchmark my custom code. Sklearn's GridSearchCV api is making Train Accuracy %100 but Test Accuracy still remains at %69. I have nearly same results on test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  1.0\n",
      "TEST:  0.6867469879518072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# Fit the classifier to the data\n",
    "\n",
    "label_x = np.empty(len(norm_train))\n",
    "test_label_x = np.empty(len(norm_test))\n",
    "for x in range(0, len(train_numpy), 1):\n",
    "    label_x[x] = train_numpy[x][0]\n",
    "\n",
    "for x in range(0, len(test_numpy), 1):\n",
    "    test_label_x[x] = test_numpy[x][0]\n",
    "\n",
    "awesome = knn.fit(train_numpy, label_x)\n",
    "\n",
    "param_grid = {\"n_neighbors\": np.arange(1, 25)}\n",
    "knn_gscv = GridSearchCV(knn, param_grid, cv=5)\n",
    "# fit model to data\n",
    "knn_gscv.fit(train_numpy, label_x)\n",
    "print(\"TRAIN: \", knn.score(train_numpy, label_x))\n",
    "print(\"TEST: \", knn.score(test_numpy, test_label_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS - KNN - Euclidian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| K-Size \t| TrainData \t| TestData \t| TrainAccuracy (%) TP(KFold-5) \t| TrainAccuracy (%)  All Data \t| TestAccuracy(%) TP \t| NormalizedData \t|\n",
    "|:------:\t|:---------:\t|:--------:\t|:-----------------------------:\t|:---------------------------:\t|:------------------:\t|:--------------:\t|\n",
    "| 1 \t| 249 \t| 81 \t| % 61.46 \t| %100 \t| % 56.79 \t| NO \t|\n",
    "| 3 \t| 249 \t| 81 \t| % 53.01 \t| % 78.71 \t| % 51.85 \t| NO \t|\n",
    "| 5 \t| 249 \t| 81 \t| % 52.22 \t| % 75.10 \t| % 55.55 \t| NO \t|\n",
    "| 7 \t| 249 \t| 81 \t| % 48.99 \t| % 69.07 \t| % 54.32 \t| NO \t|\n",
    "| 9 \t| 249 \t| 81 \t| % 44.58 \t| % 65.86 \t| % 50.61 \t| NO \t|\n",
    "| 11 \t| 249 \t| 81 \t| % 43.78 \t| % 61.84 \t| % 50.61 \t| NO \t|\n",
    "| 13 \t| 249 \t| 81 \t| % 42.17 \t| % 61.04 \t| % 50.61 \t| NO \t|\n",
    "| 1 \t| 249 \t| 81 \t| % 60.66 \t| % 60.49 \t| % 100 \t| YES \t|\n",
    "| 3 \t| 249 \t| 81 \t| % 55.41 \t| % 77.10 \t| % 62.96 \t| YES \t|\n",
    "| 5 \t| 249 \t| 81 \t| % 53.02 \t| % 68.67 \t| % 68.67 \t| YES \t|\n",
    "| 7 \t| 249 \t| 81 \t| % 50.98 \t| % 64.65 \t| % 61.72 \t| YES \t|\n",
    "| 9 \t| 249 \t| 81 \t| % 47.80 \t| % 63.05 \t| % 59.25 \t| YES \t|\n",
    "| 11 \t| 249 \t| 81 \t| % 45.00 \t| % 59.43 \t| % 51.85 \t| YES \t|\n",
    "| 13 \t| 249 \t| 81 \t| % 43.79 \t| % 59.43 \t| % 46.91 \t| YES \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "Without data normalization we have better training results Train:%100, Test:%56 but with data normalization we more general model. Train:%77 , Test%62 with k=1. Data normalization is very important for generalizing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 KNN with Manhattan Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the function. I have just changed the distance formula euclidian to manhattan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: 5\n",
      "K SIZE : 1 MEAN ACC  % 88.35918367346939\n",
      "K SIZE : 3 MEAN ACC  % 69.90204081632653\n",
      "K SIZE : 5 MEAN ACC  % 72.29387755102042\n",
      "K SIZE : 7 MEAN ACC  % 59.436734693877554\n",
      "K SIZE : 9 MEAN ACC  % 63.83673469387755\n",
      "K SIZE : 11 MEAN ACC  % 62.63673469387756\n",
      "K SIZE : 13 MEAN ACC  % 55.81224489795918\n",
      "{1: 88.35918367346939, 3: 69.90204081632653, 5: 72.29387755102042, 7: 59.436734693877554, 9: 63.83673469387755, 11: 62.63673469387756, 13: 55.81224489795918}\n",
      "Test1\n",
      "Accuracy Rate % 86.41975308641975\n",
      "Train1\n",
      "Accuracy Rate % 100.0\n",
      "----------------------\n",
      "\n",
      "Test3\n",
      "Accuracy Rate % 70.37037037037037\n",
      "Train3\n",
      "Accuracy Rate % 100.0\n",
      "----------------------\n",
      "\n",
      "Test5\n",
      "Accuracy Rate % 76.54320987654322\n",
      "Train5\n",
      "Accuracy Rate % 98.39357429718875\n",
      "----------------------\n",
      "\n",
      "Test7\n",
      "Accuracy Rate % 70.37037037037037\n",
      "Train7\n",
      "Accuracy Rate % 96.3855421686747\n",
      "----------------------\n",
      "\n",
      "Test9\n",
      "Accuracy Rate % 71.60493827160494\n",
      "Train9\n",
      "Accuracy Rate % 97.59036144578313\n",
      "----------------------\n",
      "\n",
      "Test11\n",
      "Accuracy Rate % 71.60493827160494\n",
      "Train11\n",
      "Accuracy Rate % 95.18072289156626\n",
      "----------------------\n",
      "\n",
      "Test13\n",
      "Accuracy Rate % 69.1358024691358\n",
      "Train13\n",
      "Accuracy Rate % 96.3855421686747\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_best_k_value(train_numpy):\n",
    "    train_results = {}\n",
    "    split_size = 5\n",
    "    kf = KFold(n_splits=split_size, random_state=False, shuffle=True)\n",
    "\n",
    "    splits = kf.get_n_splits(train_numpy)\n",
    "\n",
    "    print(\"Splits: \" + str(splits))\n",
    "\n",
    "    for k_size in range(1, 15, 2):\n",
    "        mean_pos = 0\n",
    "        temp_acc = 0\n",
    "       \n",
    "        for train_index, test_index in kf.split(train_numpy):\n",
    "            # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train = []\n",
    "            X_test = []\n",
    "            X_train, X_test = train_numpy[train_index], train_numpy[test_index]\n",
    "\n",
    "            positive = 0\n",
    "            negative = 0\n",
    "            for x in range(0, len(X_test)):\n",
    "                sorted2, neigbours2 = data_util.k_nearest_neighbourhood_manhattan(X_train, X_test[x], k_size)\n",
    "                if int(X_test[x][0]) == int(sorted2):\n",
    "                    positive += 1\n",
    "                else:\n",
    "                    negative += 1\n",
    "\n",
    "            temp_acc = (100 * positive) / (positive + negative)\n",
    "            mean_pos += temp_acc\n",
    "            #print(\"TrainSize: \" + str(len(X_train)) + \" TestSize:\" + str(len(X_test)))\n",
    "        # print(\"K : \" + str(k_size) + \" True:\" + str(positive) + \" False: \" + str(negative) + \"  Positive Accuracy Rate % \" + str(\n",
    "        # (100 * positive) / (positive + negative)))\n",
    "        train_results[k_size] = mean_pos / split_size\n",
    "        # print(\"K : \" + str(k_size) + \" True:\" + str(positive) + \" False: \" + str(negative) + \"  Negative Accuracy Rate % \" + str(\n",
    "        # (100 * negative) / (positive + negative)))\n",
    "\n",
    "        print(\"K SIZE : \" + str(k_size) + \" MEAN ACC  % \" + str(mean_pos / split_size))\n",
    "    return train_results\n",
    "\n",
    "print(find_best_k_value(norm_train))\n",
    "\n",
    "\n",
    "def calcTestScore(X_train, X_test,k_size):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for x in range(0, len(X_test)):\n",
    "        sorted2, neigbours2 = data_util.k_nearest_neighbourhood_manhattan(X_train, X_test[x],k_size)\n",
    "        if int(X_test[x][0]) == int(sorted2):\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    print( \"Accuracy Rate % \" + str((100 * positive) / (positive + negative)))\n",
    "\n",
    "for index in range(1,15,2):\n",
    "    print(\"Test\"+ str(index))\n",
    "    calcTestScore(norm_train, norm_test,index)\n",
    "    print(\"Train\"+ str(index))\n",
    "    calcTestScore(norm_train, norm_train,index)\n",
    "    print(\"----------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS - KNN - Manhattan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| K-Size \t| TrainData \t| TestData \t| TrainAccuracy (%) TP(KFold-5) \t| TrainAccuracy (%)  All Data \t| TestAccuracy(%) TP \t| NormalizedData \t|\n",
    "|:------:\t|:---------:\t|:--------:\t|:-----------------------------:\t|:---------------------------:\t|:------------------:\t|:--------------:\t|\n",
    "| 1 \t| 249 \t| 81 \t| % 88.35 \t| % 100.0 \t| % 86.41 \t| YES \t|\n",
    "| 3 \t| 249 \t| 81 \t| % 69.90 \t| % 100.0 \t| % 70.37 \t| YES \t|\n",
    "| 5 \t| 249 \t| 81 \t| % 72.29 \t| % 98.39 \t| % 76.54 \t| YES \t|\n",
    "| 7 \t| 249 \t| 81 \t| % 59.43 \t| % 96.38 \t| % 70.37 \t| YES \t|\n",
    "| 9 \t| 249 \t| 81 \t| % 63.83 \t| % 97.59 \t| % 71.60 \t| YES \t|\n",
    "| 11 \t| 249 \t| 81 \t| % 62.63 \t| % 95.18 \t| % 71.60 \t| YES \t|\n",
    "| 13 \t| 249 \t| 81 \t| % 55.81 \t| % 96.38 \t| % 69.13 \t| YES \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "Using Manhattan distance as metric increased our test accuracy (K=1) **%60** to **%86**. (Normalized data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3 Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use SKLearn's SVM implementation. Sklearn needs labels and data separately. So we need to separate our data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Separation\n",
    "Delete first two column and separate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  257\n",
      "Test:  83\n",
      "(257, 15)\n",
      "(83, 15)\n"
     ]
    }
   ],
   "source": [
    "training_data, testing_data = data_util.split_training_and_testing_data(data)\n",
    "\n",
    "total_train = 0\n",
    "for x in range(0,len(training_data)):\n",
    "    total_train += len(training_data[x])\n",
    "\n",
    "print(\"Train: \", total_train )\n",
    "total_test = 0\n",
    "for x in range(0,len(testing_data)):\n",
    "    total_test += len(testing_data[x])\n",
    "print(\"Test: \", total_test )\n",
    "\n",
    "train_numpy = np.asarray(training_data[0])\n",
    "test_numpy = np.asarray(testing_data[0])\n",
    "for x in range(1,len(training_data)):\n",
    "    train_numpy = np.concatenate((train_numpy,np.asarray(training_data[x])), axis=0 )\n",
    "    \n",
    "for x in range(1,len(testing_data)):\n",
    "    test_numpy = np.concatenate((test_numpy,np.asarray(testing_data[x])), axis=0 )\n",
    "\n",
    "\n",
    "#print(test_numpy)\n",
    "print(train_numpy.shape)\n",
    "print(test_numpy.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "\n",
    "def delete_first_two_column(data_array):\n",
    "    label_array = np.empty(len(data_array))\n",
    "    for index_label in range(0, len(data_array), 1):\n",
    "        #print(\"Doing:\" + str(index_label))\n",
    "        #print(\"DATA: \" + str(data_array[index_label][0]))\n",
    "        label_array[index_label] = data_array[index_label][0]\n",
    "    #comment this line if you using svm or decision tree.\n",
    "    data_array = np.delete(data_array, np.s_[0:2], axis=1)\n",
    "\n",
    "    return data_array, label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.2694e-01 1.4742e+00 3.2396e-01 9.8535e-01 1.0000e+00 8.3592e-01\n",
      " 4.6566e-03 3.9465e-03 4.7790e-02 1.2795e-01 1.6108e-02 5.2323e-03\n",
      " 2.7477e-04]\n",
      "1.0\n",
      "13\n",
      "257\n"
     ]
    }
   ],
   "source": [
    "## delete first two rows for each test and train data.\n",
    "train_numpy, train_labels = delete_first_two_column(train_numpy)\n",
    "test_numpy, test_labels = delete_first_two_column(test_numpy)\n",
    "\n",
    "\n",
    "print(train_numpy[0])\n",
    "print(train_labels[0])\n",
    "\n",
    "print(len(train_numpy[0]))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]\n",
      "###---- TRAIN DATA SCORE ----####\n",
      "0.8521400778210116\n",
      "###--------------------------####\n",
      "###---- TEST DATA SCORE ----####\n",
      "0.7831325301204819\n",
      "####################\n",
      "[LibLinear]0.8146341463414634\n",
      "0.5961538461538461\n",
      "####################\n",
      "[LibLinear]0.8195121951219512\n",
      "0.7307692307692307\n",
      "####################\n",
      "[LibLinear]0.8252427184466019\n",
      "0.6666666666666666\n",
      "####################\n",
      "[LibLinear]0.8495145631067961\n",
      "0.6666666666666666\n",
      "####################\n",
      "[LibLinear]0.8495145631067961\n",
      "0.5294117647058824\n",
      "AVERAGE RESULTS\n",
      "0.8316836372247216\n",
      "0.6379336349924585\n",
      "{0: 0.8146341463414634, 1: 0.8195121951219512, 2: 0.8252427184466019, 3: 0.8495145631067961, 4: 0.8495145631067961}\n",
      "{0: 0.5961538461538461, 1: 0.7307692307692307, 2: 0.6666666666666666, 3: 0.6666666666666666, 4: 0.5294117647058824}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics, svm\n",
    " \n",
    "kf = KFold(n_splits=5, random_state=False, shuffle=True)\n",
    "\n",
    "clf = svm.LinearSVC(penalty='l2', verbose=2, max_iter=100000, class_weight=None)\n",
    "\n",
    "clf.fit(train_numpy, train_labels, sample_weight=None)\n",
    "\n",
    "# print(str(train_numpy))\n",
    "#print(str(test_labels))\n",
    "\n",
    "print(\"\\n###---- TRAIN DATA SCORE ----####\")\n",
    "print(clf.score(train_numpy, train_labels))\n",
    "print(\"###--------------------------####\")\n",
    "print(\"###---- TEST DATA SCORE ----####\")\n",
    "print(clf.score(test_numpy, test_labels))\n",
    "\n",
    "average_train=0\n",
    "average_test=0\n",
    "train_results_dict= {}\n",
    "test_results_dict={}\n",
    "index_dict= 0\n",
    "for train_index, test_index in kf.split(train_numpy):\n",
    "    print(\"####################\")\n",
    "    \n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = train_numpy[train_index], train_numpy[test_index]\n",
    "    X_train_labels, X_test_labels = train_labels[train_index], train_labels[test_index]\n",
    "    clf.fit(X_train, X_train_labels)\n",
    "    train_results_dict[index_dict] = clf.score(X_train, X_train_labels)\n",
    "    test_results_dict[index_dict] = clf.score(X_test, X_test_labels)\n",
    "    average_train += clf.score(X_train, X_train_labels)\n",
    "    average_test += clf.score(X_test, X_test_labels)\n",
    "    index_dict +=1\n",
    "    \n",
    "    \n",
    "    print(clf.score(X_train, X_train_labels))\n",
    "    print(clf.score(X_test, X_test_labels))\n",
    "\n",
    "print(\"AVERAGE RESULTS\")\n",
    "print(str(average_train / 5))\n",
    "print(str(average_test / 5))\n",
    "\n",
    "\n",
    "print(train_results_dict)\n",
    "print(test_results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(norm_data):\n",
    "    for x in range(0, len(norm_data), 1):\n",
    "        temp_point = norm_data[x]\n",
    "        # print(\"BEFORE_NORM\")\n",
    "        # print(temp_point)\n",
    "        total_sum = sum(temp_point)\n",
    "        for y in range(0, len(temp_point)):\n",
    "            temp_point[y] = temp_point[y] / total_sum\n",
    "\n",
    "        # print(\"AFTER_NORM\")\n",
    "        # print(temp_point)\n",
    "        norm_data[x] = temp_point\n",
    "    \n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_data = normalizeData(train_numpy)\n",
    "norm_test_data = normalizeData(test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]###---- TRAIN DATA SCORE ----####\n",
      "0.09727626459143969\n",
      "###--------------------------####\n",
      "###---- TEST DATA SCORE ----####\n",
      "0.07228915662650602\n",
      "####################\n",
      "[LibLinear]0.33170731707317075\n",
      "0.15384615384615385\n",
      "####################\n",
      "[LibLinear]0.2926829268292683\n",
      "0.15384615384615385\n",
      "####################\n",
      "[LibLinear]0.3106796116504854\n",
      "0.19607843137254902\n",
      "####################\n",
      "[LibLinear]0.3058252427184466\n",
      "0.1568627450980392\n",
      "####################\n",
      "[LibLinear]0.2669902912621359\n",
      "0.19607843137254902\n",
      "AVERAGE RESULTS\n",
      "0.3015770779067014\n",
      "0.171342383107089\n",
      "{0: 0.33170731707317075, 1: 0.2926829268292683, 2: 0.3106796116504854, 3: 0.3058252427184466, 4: 0.2669902912621359}\n",
      "{0: 0.15384615384615385, 1: 0.15384615384615385, 2: 0.19607843137254902, 3: 0.1568627450980392, 4: 0.19607843137254902}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clf_norm = svm.LinearSVC(penalty='l2', verbose=1, max_iter=10000, class_weight=None)\n",
    "\n",
    "clf_norm.fit(norm_train_data, train_labels, sample_weight=None)\n",
    "\n",
    "# print(str(train_numpy))\n",
    "#print(str(test_labels))\n",
    "\n",
    "print(\"###---- TRAIN DATA SCORE ----####\")\n",
    "print(clf.score(norm_train_data, train_labels))\n",
    "print(\"###--------------------------####\")\n",
    "print(\"###---- TEST DATA SCORE ----####\")\n",
    "print(clf.score(norm_test_data, test_labels))\n",
    "\n",
    "average_train=0\n",
    "average_test=0\n",
    "train_results_dict= {}\n",
    "test_results_dict={}\n",
    "index_dict= 0\n",
    "for train_index, test_index in kf.split(norm_train_data):\n",
    "    print(\"####################\")\n",
    "    \n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = norm_train_data[train_index], norm_train_data[test_index]\n",
    "    X_train_labels, X_test_labels = train_labels[train_index], train_labels[test_index]\n",
    "    clf_norm.fit(X_train, X_train_labels)\n",
    "    train_results_dict[index_dict] = clf_norm.score(X_train, X_train_labels)\n",
    "    test_results_dict[index_dict] = clf_norm.score(X_test, X_test_labels)\n",
    "    average_train += clf_norm.score(X_train, X_train_labels)\n",
    "    average_test += clf_norm.score(X_test, X_test_labels)\n",
    "    index_dict +=1\n",
    "    \n",
    "    \n",
    "    print(clf_norm.score(X_train, X_train_labels))\n",
    "    print(clf_norm.score(X_test, X_test_labels))\n",
    "\n",
    "print(\"AVERAGE RESULTS\")\n",
    "print(str(average_train / 5))\n",
    "print(str(average_test / 5))\n",
    "\n",
    "\n",
    "print(train_results_dict)\n",
    "print(test_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS - Linear SVM\n",
    "| Fold \t| TrainData \t| TestData \t| TrainAccuracy (%) TP \t| TestAccuracy(%) TP \t| NormalizedData \t|\n",
    "|:----:\t|:---------:\t|:--------:\t|:--------------------:\t|:------------------:\t|:--------------:\t|\n",
    "| 1 \t| 249 \t| 81 \t| % 63.03 \t| % 57.83 \t| NO \t|\n",
    "| 2 \t| 249 \t| 81 \t| % 63.41 \t| % 51.92 \t| NO \t|\n",
    "| 3 \t| 249 \t| 81 \t| % 59.02 \t| % 34.61 \t| NO \t|\n",
    "| 4 \t| 249 \t| 81 \t| % 63.10 \t| % 45.09 \t| NO \t|\n",
    "| 5 \t| 249 \t| 81 \t| % 64.07 \t| % 37.25 \t| NO \t|\n",
    "| ALL \t| 249 \t| 81 \t| % 62.54 \t| % 44.36 \t| NO \t|\n",
    "| ALL \t| 249 \t| 81 \t| % 30.15 \t| % 17.13 \t| YES \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "In Linear SVM, normalized data training results are awful. Not normalized data training results are much better than this. Best results are (Train **%63 and Test %57**). Incresing max iterations didn't change the results.L2 penalty is improved the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4 Polynomial SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8482490272373541\n",
      "0.8192771084337349\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "AVERAGE KFOLD RESULTS1\n",
      "0.7772341937011602\n",
      "0.6065610859728506\n",
      "1.0\n",
      "1.0\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "AVERAGE KFOLD RESULTS2\n",
      "1.0\n",
      "0.996078431372549\n",
      "1.0\n",
      "0.9879518072289156\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "AVERAGE KFOLD RESULTS3\n",
      "0.9990243902439024\n",
      "1.0\n",
      "1.0\n",
      "0.9759036144578314\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "AVERAGE KFOLD RESULTS4\n",
      "0.9990243902439024\n",
      "0.996078431372549\n",
      "1.0\n",
      "0.9397590361445783\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "AVERAGE KFOLD RESULTS5\n",
      "0.9990243902439024\n",
      "0.9883861236802414\n",
      "1.0\n",
      "0.927710843373494\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "AVERAGE KFOLD RESULTS6\n",
      "0.9990243902439024\n",
      "0.9845399698340875\n",
      "1.0\n",
      "0.927710843373494\n",
      "####################\n",
      "####################\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "####################\n",
      "AVERAGE KFOLD RESULTS7\n",
      "0.9990243902439024\n",
      "0.9766968325791856\n",
      "0.9961089494163424\n",
      "0.9036144578313253\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "AVERAGE KFOLD RESULTS8\n",
      "0.9970779067013972\n",
      "0.9689291101055806\n",
      "0.9961089494163424\n",
      "0.9036144578313253\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "AVERAGE KFOLD RESULTS9\n",
      "0.9970779067013972\n",
      "0.9766214177978885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for degrees in range(1,10,1):\n",
    "    poly_svm = svm.SVC(kernel=\"poly\", degree=degrees)\n",
    "\n",
    "    poly_svm.fit(train_numpy, train_labels)\n",
    "\n",
    "    #print(\"POLY NOT MIXED RESULTS\")\n",
    "    #print(len(train_numpy))\n",
    "    #print(len(test_numpy))\n",
    "\n",
    "    print(poly_svm.score(train_numpy, train_labels))\n",
    "    print(poly_svm.score(test_numpy, test_labels))\n",
    "\n",
    "    average_train = 0\n",
    "    average_test = 0\n",
    "    for train_index, test_index in kf.split(train_numpy):\n",
    "        print(\"####################\")\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = train_numpy[train_index], train_numpy[test_index]\n",
    "        X_train_labels, X_test_labels = train_labels[train_index], train_labels[test_index]\n",
    "        poly_svm.fit(X_train, X_train_labels)\n",
    "        average_train += poly_svm.score(X_train, X_train_labels)\n",
    "        average_test += poly_svm.score(X_test, X_test_labels)\n",
    "        #print(poly_svm.score(X_train, X_train_labels))\n",
    "        #print(poly_svm.score(X_test, X_test_labels))\n",
    "\n",
    "    print(\"AVERAGE KFOLD RESULTS\"+ str(degrees))\n",
    "    print(str(average_train / 5))\n",
    "    print(str(average_test / 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_data = normalizeData(train_numpy)\n",
    "norm_test_data = normalizeData(test_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLY NOT MIXED RESULTS\n",
      "0.04669260700389105\n",
      "0.04819277108433735\n",
      "####################\n",
      "0.04878048780487805\n",
      "0.038461538461538464\n",
      "####################\n",
      "0.08780487804878048\n",
      "0.038461538461538464\n",
      "####################\n",
      "0.16990291262135923\n",
      "0.0196078431372549\n",
      "####################\n",
      "0.04854368932038835\n",
      "0.0392156862745098\n",
      "####################\n",
      "0.07766990291262135\n",
      "0.0392156862745098\n",
      "AVERAGE KFOLD RESULTS\n",
      "0.0865403741416055\n",
      "0.03499245852187029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "poly_svm_norm = svm.SVC(kernel=\"poly\", degree=3)\n",
    "\n",
    "poly_svm_norm.fit(norm_train_data, train_labels)\n",
    "\n",
    "print(\"POLY NOT MIXED RESULTS\")\n",
    "\n",
    "print(poly_svm_norm.score(norm_train_data, train_labels))\n",
    "print(poly_svm_norm.score(norm_test_data, test_labels))\n",
    "\n",
    "average_train = 0\n",
    "average_test = 0\n",
    "for train_index, test_index in kf.split(norm_train_data):\n",
    "    print(\"####################\")\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = norm_train_data[train_index], norm_train_data[test_index]\n",
    "    X_train_labels, X_test_labels = train_labels[train_index], train_labels[test_index]\n",
    "    poly_svm_norm.fit(X_train, X_train_labels)\n",
    "    average_train += poly_svm_norm.score(X_train, X_train_labels)\n",
    "    average_test += poly_svm_norm.score(X_test, X_test_labels)\n",
    "    print(poly_svm_norm.score(X_train, X_train_labels))\n",
    "    print(poly_svm_norm.score(X_test, X_test_labels))\n",
    "\n",
    "print(\"AVERAGE KFOLD RESULTS\")\n",
    "print(str(average_train / 5))\n",
    "print(str(average_test / 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS - Poly SVM\n",
    "| Poly Degree \t| TrainData \t| TestData \t| TrainAccuracy (%) TP \t| TestAccuracy(%) TP \t| NormalizedData \t|\n",
    "|:-----------:\t|:---------:\t|:--------:\t|:--------------------:\t|:------------------:\t|:--------------:\t|\n",
    "| 1 \t| 257 \t| 83 \t| % 77.72 \t| % 60.65 \t| NO \t|\n",
    "| 2 \t| 257 \t| 83 \t| % 100 \t| % 99.60 \t| NO \t|\n",
    "| 3 \t| 257 \t| 83 \t| % 99.90 \t| % 97.59 \t| NO \t|\n",
    "| 4 \t| 257 \t| 83 \t| % 99.90 \t| % 99.60 \t| NO \t|\n",
    "| 5 \t| 257 \t| 83 \t| % 99.90 \t| % 98.73 \t| NO \t|\n",
    "| 6 \t| 257 \t| 83 \t| % 99.90 \t| % 98.45 \t| NO \t|\n",
    "| 50 \t| 257 \t| 83 \t| % 63.41 \t| % 51.92 \t| NO \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "The normalized result is not so good. The best results are in where at degree 2. I have used all data in SVM. It gave me better results. I didn't delete 2 rows. All csv data more fit for SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTREE RESULTS\n",
      "1.0\n",
      "0.6274509803921569\n",
      "[0.46835443 0.72058824 0.38983051]\n",
      "[0.30769231 0.5625     0.33333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "my_decision_tree = tree.DecisionTreeClassifier(criterion=\"gini\", class_weight=\"balanced\")\n",
    "\n",
    "my_decision_tree.fit(X_train, X_train_labels)\n",
    "\n",
    "print(\"DTREE RESULTS\")\n",
    "\n",
    "print(my_decision_tree.score(X_train, X_train_labels))\n",
    "print(my_decision_tree.score(X_test, X_test_labels))\n",
    "\n",
    "print(cross_val_score(my_decision_tree, X_train, X_train_labels, cv=3))\n",
    "print(cross_val_score(my_decision_tree, X_test, X_test_labels, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8737864077669902\n",
      "0.7254901960784313\n",
      "206\n",
      "206\n",
      "[0.49367089 0.52941176 0.38983051]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dtree = tree.DecisionTreeClassifier(class_weight=\"balanced\", criterion='gini', max_depth=150,\n",
    "                                    max_features=13, max_leaf_nodes=50, min_samples_leaf=1,\n",
    "                                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                    presort=False, random_state=None, splitter='random')\n",
    "dtree.fit(X_train, X_train_labels)\n",
    "\n",
    "print(dtree.score(X_train, X_train_labels))\n",
    "print(dtree.score(X_test, X_test_labels))\n",
    "\n",
    "print(len(X_train_labels))\n",
    "print(len(X_train))\n",
    "\n",
    "print(cross_val_score(dtree,X_train,X_train_labels, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.6078431372549019\n",
      "206\n",
      "206\n",
      "[0.48101266 0.55882353 0.3220339 ]\n"
     ]
    }
   ],
   "source": [
    "## diff params\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dtree = tree.DecisionTreeClassifier(class_weight=\"balanced\", criterion='gini', max_depth=100,\n",
    "                                    max_features=9, max_leaf_nodes=100, min_samples_leaf=1,\n",
    "                                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                    presort=False, random_state=None, splitter='random')\n",
    "dtree.fit(X_train, X_train_labels)\n",
    "\n",
    "print(dtree.score(X_train, X_train_labels))\n",
    "print(dtree.score(X_test, X_test_labels))\n",
    "\n",
    "print(len(X_train_labels))\n",
    "print(len(X_train))\n",
    "\n",
    "print(cross_val_score(dtree,X_train,X_train_labels, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6650485436893204\n",
      "0.4117647058823529\n",
      "206\n",
      "206\n",
      "[0.30379747 0.5        0.33898305]\n"
     ]
    }
   ],
   "source": [
    "## diff params\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dtree = tree.DecisionTreeClassifier(class_weight=\"balanced\", criterion='gini', max_depth=150,\n",
    "                                    max_features=11, max_leaf_nodes=25, min_samples_leaf=1,\n",
    "                                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                    presort=False, random_state=None, splitter='random')\n",
    "dtree.fit(X_train, X_train_labels)\n",
    "\n",
    "print(dtree.score(X_train, X_train_labels))\n",
    "print(dtree.score(X_test, X_test_labels))\n",
    "\n",
    "print(len(X_train_labels))\n",
    "print(len(X_train))\n",
    "\n",
    "print(cross_val_score(dtree,X_train,X_train_labels, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5882352941176471\n",
      "206\n",
      "206\n",
      "[0.40506329 0.69117647 0.45762712]\n"
     ]
    }
   ],
   "source": [
    "## diff params\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dtree = tree.DecisionTreeClassifier(class_weight=\"balanced\", criterion='gini', max_depth=150,\n",
    "                                    max_features=13, max_leaf_nodes=200, min_samples_leaf=1,\n",
    "                                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                    presort=False, random_state=None, splitter='random')\n",
    "dtree.fit(X_train, X_train_labels)\n",
    "\n",
    "print(dtree.score(X_train, X_train_labels))\n",
    "print(dtree.score(X_test, X_test_labels))\n",
    "\n",
    "print(len(X_train_labels))\n",
    "print(len(X_train))\n",
    "\n",
    "print(cross_val_score(dtree,X_train,X_train_labels, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7135922330097088\n",
      "0.45098039215686275\n",
      "206\n",
      "206\n",
      "[0.44303797 0.5        0.47457627]\n"
     ]
    }
   ],
   "source": [
    "## diff params\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dtree = tree.DecisionTreeClassifier(class_weight=\"balanced\", criterion='gini', max_depth=30,\n",
    "                                    max_features=13, max_leaf_nodes=30, min_samples_leaf=3,\n",
    "                                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                    presort=False, random_state=None, splitter='random')\n",
    "dtree.fit(X_train, X_train_labels)\n",
    "\n",
    "print(dtree.score(X_train, X_train_labels))\n",
    "print(dtree.score(X_test, X_test_labels))\n",
    "\n",
    "print(len(X_train_labels))\n",
    "print(len(X_train))\n",
    "\n",
    "print(cross_val_score(dtree,X_train,X_train_labels, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6699029126213593\n",
      "0.39215686274509803\n",
      "206\n",
      "206\n",
      "[0.39240506 0.47058824 0.3220339 ]\n"
     ]
    }
   ],
   "source": [
    "## diff params\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dtree = tree.DecisionTreeClassifier(class_weight=\"balanced\", criterion='gini', max_depth=1000,\n",
    "                                    max_features=13, max_leaf_nodes=1000, min_samples_leaf=5,\n",
    "                                    min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
    "                                    presort=False, random_state=None, splitter='random')\n",
    "dtree.fit(X_train, X_train_labels)\n",
    "\n",
    "print(dtree.score(X_train, X_train_labels))\n",
    "print(dtree.score(X_test, X_test_labels))\n",
    "\n",
    "print(len(X_train_labels))\n",
    "print(len(X_train))\n",
    "\n",
    "print(cross_val_score(dtree,X_train,X_train_labels, cv=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tree(tree, feature_names):\n",
    "    left = tree.tree_.children_left\n",
    "    right = tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features = [feature_names[i] for i in tree.tree_.feature]\n",
    "\n",
    "    # get ids of child nodes\n",
    "    idx = np.argwhere(left == -1)[:, 0]\n",
    "\n",
    "    def recurse(left, right, child, lineage=None):\n",
    "        if lineage is None:\n",
    "            lineage = [child]\n",
    "        if child in left:\n",
    "            parent = np.where(left == child)[0].item()\n",
    "            split = 'l'\n",
    "        else:\n",
    "            parent = np.where(right == child)[0].item()\n",
    "            split = 'r'\n",
    "\n",
    "        lineage.append((parent, split, threshold[parent], features[parent]))\n",
    "\n",
    "        if parent == 0:\n",
    "            lineage.reverse()\n",
    "            return lineage\n",
    "        else:\n",
    "            return recurse(left, right, parent, lineage)\n",
    "\n",
    "    for child in idx:\n",
    "        for node in recurse(left, right, child):\n",
    "            print(node)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'l', 0.019514335319399834, 1.0)\n",
      "(2, 'l', 0.0138433831743896, 1.0)\n",
      "3\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'l', 0.019514335319399834, 1.0)\n",
      "(2, 'r', 0.0138433831743896, 1.0)\n",
      "(4, 'l', 0.0007800880412105471, 1.0)\n",
      "(5, 'l', 0.019926609471440315, 1.0)\n",
      "6\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'l', 0.019514335319399834, 1.0)\n",
      "(2, 'r', 0.0138433831743896, 1.0)\n",
      "(4, 'l', 0.0007800880412105471, 1.0)\n",
      "(5, 'r', 0.019926609471440315, 1.0)\n",
      "7\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'l', 0.019514335319399834, 1.0)\n",
      "(2, 'r', 0.0138433831743896, 1.0)\n",
      "(4, 'r', 0.0007800880412105471, 1.0)\n",
      "8\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'l', 0.0011780084460042417, 2.0)\n",
      "10\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'l', 0.18758953362703323, 1.0)\n",
      "(12, 'l', 0.11888254806399345, 1.0)\n",
      "13\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'l', 0.18758953362703323, 1.0)\n",
      "(12, 'r', 0.11888254806399345, 1.0)\n",
      "(14, 'l', 0.00044340668682707474, 2.0)\n",
      "15\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'l', 0.18758953362703323, 1.0)\n",
      "(12, 'r', 0.11888254806399345, 1.0)\n",
      "(14, 'r', 0.00044340668682707474, 2.0)\n",
      "16\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'l', 0.01926423143595457, 1.0)\n",
      "(23, 'l', 0.014731833711266518, 1.0)\n",
      "24\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'l', 0.01926423143595457, 1.0)\n",
      "(23, 'r', 0.014731833711266518, 1.0)\n",
      "(25, 'l', 0.011083698831498623, 1.0)\n",
      "26\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'l', 0.01926423143595457, 1.0)\n",
      "(23, 'r', 0.014731833711266518, 1.0)\n",
      "(25, 'r', 0.011083698831498623, 1.0)\n",
      "(27, 'l', 0.013145812321454287, 1.0)\n",
      "28\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'l', 0.01926423143595457, 1.0)\n",
      "(23, 'r', 0.014731833711266518, 1.0)\n",
      "(25, 'r', 0.011083698831498623, 1.0)\n",
      "(27, 'r', 0.013145812321454287, 1.0)\n",
      "(29, 'l', 0.6807066798210144, 1.0)\n",
      "(30, 'l', 0.021267451345920563, 1.0)\n",
      "(31, 'l', 0.21183095127344131, 1.0)\n",
      "32\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'l', 0.01926423143595457, 1.0)\n",
      "(23, 'r', 0.014731833711266518, 1.0)\n",
      "(25, 'r', 0.011083698831498623, 1.0)\n",
      "(27, 'r', 0.013145812321454287, 1.0)\n",
      "(29, 'l', 0.6807066798210144, 1.0)\n",
      "(30, 'l', 0.021267451345920563, 1.0)\n",
      "(31, 'r', 0.21183095127344131, 1.0)\n",
      "33\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'l', 0.01926423143595457, 1.0)\n",
      "(23, 'r', 0.014731833711266518, 1.0)\n",
      "(25, 'r', 0.011083698831498623, 1.0)\n",
      "(27, 'r', 0.013145812321454287, 1.0)\n",
      "(29, 'l', 0.6807066798210144, 1.0)\n",
      "(30, 'r', 0.021267451345920563, 1.0)\n",
      "34\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'l', 0.01926423143595457, 1.0)\n",
      "(23, 'r', 0.014731833711266518, 1.0)\n",
      "(25, 'r', 0.011083698831498623, 1.0)\n",
      "(27, 'r', 0.013145812321454287, 1.0)\n",
      "(29, 'r', 0.6807066798210144, 1.0)\n",
      "35\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'r', 0.01926423143595457, 1.0)\n",
      "(36, 'l', 0.00038819316250737756, 2.0)\n",
      "(37, 'l', 0.008070311043411493, 1.0)\n",
      "38\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'r', 0.01926423143595457, 1.0)\n",
      "(36, 'l', 0.00038819316250737756, 2.0)\n",
      "(37, 'r', 0.008070311043411493, 1.0)\n",
      "39\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'r', 0.01926423143595457, 1.0)\n",
      "(36, 'r', 0.00038819316250737756, 2.0)\n",
      "(40, 'l', 0.0025627724826335907, 2.0)\n",
      "41\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'r', 0.01926423143595457, 1.0)\n",
      "(36, 'r', 0.00038819316250737756, 2.0)\n",
      "(40, 'r', 0.0025627724826335907, 2.0)\n",
      "(42, 'l', 0.6853466033935547, 1.0)\n",
      "43\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'r', 0.01926423143595457, 1.0)\n",
      "(36, 'r', 0.00038819316250737756, 2.0)\n",
      "(40, 'r', 0.0025627724826335907, 2.0)\n",
      "(42, 'r', 0.6853466033935547, 1.0)\n",
      "(44, 'l', 0.030387467704713345, 1.0)\n",
      "45\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'l', 0.00382138229906559, 2.0)\n",
      "(22, 'r', 0.01926423143595457, 1.0)\n",
      "(36, 'r', 0.00038819316250737756, 2.0)\n",
      "(40, 'r', 0.0025627724826335907, 2.0)\n",
      "(42, 'r', 0.6853466033935547, 1.0)\n",
      "(44, 'r', 0.030387467704713345, 1.0)\n",
      "46\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'l', 0.014371122233569622, 1.0)\n",
      "(21, 'r', 0.00382138229906559, 2.0)\n",
      "47\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'r', 0.014371122233569622, 1.0)\n",
      "(48, 'l', 8.528859325451776e-06, 2.0)\n",
      "49\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'r', 0.014371122233569622, 1.0)\n",
      "(48, 'r', 8.528859325451776e-06, 2.0)\n",
      "(50, 'l', 0.001152597222244367, 1.0)\n",
      "51\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'l', 0.005636079935356975, 2.0)\n",
      "(20, 'r', 0.014371122233569622, 1.0)\n",
      "(48, 'r', 8.528859325451776e-06, 2.0)\n",
      "(50, 'r', 0.001152597222244367, 1.0)\n",
      "52\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'r', 0.005636079935356975, 2.0)\n",
      "(53, 'l', 0.040292978286743164, 1.0)\n",
      "54\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'l', 0.03290272876620293, 1.0)\n",
      "(19, 'r', 0.005636079935356975, 2.0)\n",
      "(53, 'r', 0.040292978286743164, 1.0)\n",
      "55\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'l', 0.00027574079285841435, 2.0)\n",
      "58\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'l', 0.08425652608275414, 1.0)\n",
      "(61, 'l', 0.017514332197606564, 1.0)\n",
      "62\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'l', 0.08425652608275414, 1.0)\n",
      "(61, 'r', 0.017514332197606564, 1.0)\n",
      "(63, 'l', 0.003946808981709182, 2.0)\n",
      "(64, 'l', 0.0009877916891127825, 2.0)\n",
      "(65, 'l', 0.04191451333463192, 1.0)\n",
      "66\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'l', 0.08425652608275414, 1.0)\n",
      "(61, 'r', 0.017514332197606564, 1.0)\n",
      "(63, 'l', 0.003946808981709182, 2.0)\n",
      "(64, 'l', 0.0009877916891127825, 2.0)\n",
      "(65, 'r', 0.04191451333463192, 1.0)\n",
      "67\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'l', 0.08425652608275414, 1.0)\n",
      "(61, 'r', 0.017514332197606564, 1.0)\n",
      "(63, 'l', 0.003946808981709182, 2.0)\n",
      "(64, 'r', 0.0009877916891127825, 2.0)\n",
      "(68, 'l', 0.2824736014008522, 1.0)\n",
      "69\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'l', 0.08425652608275414, 1.0)\n",
      "(61, 'r', 0.017514332197606564, 1.0)\n",
      "(63, 'l', 0.003946808981709182, 2.0)\n",
      "(64, 'r', 0.0009877916891127825, 2.0)\n",
      "(68, 'r', 0.2824736014008522, 1.0)\n",
      "70\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'l', 0.08425652608275414, 1.0)\n",
      "(61, 'r', 0.017514332197606564, 1.0)\n",
      "(63, 'r', 0.003946808981709182, 2.0)\n",
      "(71, 'l', 0.07095876336097717, 1.0)\n",
      "72\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'l', 0.08425652608275414, 1.0)\n",
      "(61, 'r', 0.017514332197606564, 1.0)\n",
      "(63, 'r', 0.003946808981709182, 2.0)\n",
      "(71, 'r', 0.07095876336097717, 1.0)\n",
      "73\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'r', 0.08425652608275414, 1.0)\n",
      "(74, 'l', 0.045614536851644516, 1.0)\n",
      "(75, 'l', 0.0011126780300401151, 1.0)\n",
      "(76, 'l', 0.4893566370010376, 1.0)\n",
      "(77, 'l', 0.03462599031627178, 1.0)\n",
      "78\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'r', 0.08425652608275414, 1.0)\n",
      "(74, 'l', 0.045614536851644516, 1.0)\n",
      "(75, 'l', 0.0011126780300401151, 1.0)\n",
      "(76, 'l', 0.4893566370010376, 1.0)\n",
      "(77, 'r', 0.03462599031627178, 1.0)\n",
      "79\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'r', 0.08425652608275414, 1.0)\n",
      "(74, 'l', 0.045614536851644516, 1.0)\n",
      "(75, 'l', 0.0011126780300401151, 1.0)\n",
      "(76, 'r', 0.4893566370010376, 1.0)\n",
      "80\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'r', 0.08425652608275414, 1.0)\n",
      "(74, 'l', 0.045614536851644516, 1.0)\n",
      "(75, 'r', 0.0011126780300401151, 1.0)\n",
      "81\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'l', 0.034135231748223305, 1.0)\n",
      "(60, 'r', 0.08425652608275414, 1.0)\n",
      "(74, 'r', 0.045614536851644516, 1.0)\n",
      "82\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'r', 0.034135231748223305, 1.0)\n",
      "(83, 'l', 0.07836809754371643, 1.0)\n",
      "84\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'l', 0.04310267046093941, 1.0)\n",
      "(57, 'r', 0.00027574079285841435, 2.0)\n",
      "(59, 'r', 0.034135231748223305, 1.0)\n",
      "(83, 'r', 0.07836809754371643, 1.0)\n",
      "85\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'r', 0.04310267046093941, 1.0)\n",
      "(86, 'l', 0.00036767615529242903, 2.0)\n",
      "87\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'l', 0.18668773025274277, 1.0)\n",
      "(18, 'r', 0.03290272876620293, 1.0)\n",
      "(56, 'r', 0.04310267046093941, 1.0)\n",
      "(86, 'r', 0.00036767615529242903, 2.0)\n",
      "88\n",
      "(0, 'l', 0.09156093746423721, 2.0)\n",
      "(1, 'r', 0.019514335319399834, 1.0)\n",
      "(9, 'r', 0.0011780084460042417, 2.0)\n",
      "(11, 'r', 0.18758953362703323, 1.0)\n",
      "(17, 'r', 0.18668773025274277, 1.0)\n",
      "89\n",
      "(0, 'r', 0.09156093746423721, 2.0)\n",
      "(90, 'l', 0.028765808790922165, 1.0)\n",
      "91\n",
      "(0, 'r', 0.09156093746423721, 2.0)\n",
      "(90, 'r', 0.028765808790922165, 1.0)\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "draw_tree(my_decision_tree,X_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS \n",
    "My best result for decision tree: \n",
    "0.8737864077669902 - Training\n",
    "0.7254901960784313 - Testing\n",
    "\n",
    "I tried to avoid overfit. And prune the tree.\n",
    "\n",
    "| TrainData \t| TestData \t| TrainAccuracy (%) TP \t| TestAccuracy(%) TP \t| Max_Depth \t| Max_Features \t| Max_Leaf_Nodes \t| Min_Samples_Leaf \t| Min_Samples_Split \t| Criterion \t|\n",
    "|:---------:\t|:--------:\t|:--------------------:\t|:------------------:\t|:---------:\t|:------------:\t|:--------------:\t|:----------------:\t|:-----------------:\t|:---------:\t|\n",
    "| 257 \t| 83 \t| % 100 \t| % 62.74 \t| infinite \t| 13 \t| infinite \t| 1 \t| 2 \t| gini \t|\n",
    "| 257 \t| 83 \t| % 87.37 \t| % 72.54 \t| 150 \t| 13 \t| 50 \t| 1 \t| 2 \t| gini \t|\n",
    "| 257 \t| 83 \t| % 100 \t| % 60.57 \t| 100 \t| 9 \t| 100 \t| 1 \t| 2 \t| gini \t|\n",
    "| 257 \t| 83 \t| % 66.50 \t| % 41.17 \t| 150 \t| 11 \t| 25 \t| 1 \t| 2 \t| gini \t|\n",
    "| 257 \t| 83 \t| % 66.99 \t| % 39.21 \t| 1000 \t| 13 \t| 1000 \t| 5 \t| 3 \t| gini \t|\n",
    "| 257 \t| 83 \t| % 72.33 \t| % 49.01 \t| 150 \t| 13 \t| 30 \t| 3 \t| 2 \t| gini \t|\n",
    "| 257 \t| 83 \t| % 71.35 \t| % 45.09 \t| 30 \t| 13 \t| 30 \t| 3 \t| 2 \t| gini \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "Pruning helps us to avoid overfitting.\n",
    "Generally it is preferred to have a simple model, it avoids overfitting issue.\n",
    "Any additional split that does not add significant value is not worth while.\n",
    "We can avoid overfitting by changing the parameters like\n",
    "- max_leaf_nodes\n",
    "- min_samples_leaf\n",
    "- max_depth\n",
    "\n",
    "Pruning Parameters:\n",
    "\n",
    "- max_leaf_nodes\n",
    "- Reduce the number of leaf nodes\n",
    "- min_samples_leaf\n",
    "- Restrict the size of sample leaf\n",
    "- Minimum sample size in terminal nodes can be fixed to 30, 100, 300 or 5% of total\n",
    "- max_depth\n",
    "- Reduce the depth of the tree to build a generalized tree\n",
    "- Set the depth of the tree to 3, 5, 10 depending after verification on test data\n",
    "\n",
    "\n",
    "I have tried different parameters and find the best parameters as:\n",
    "\n",
    "**(class_weight=\"balanced\", criterion='gini', max_depth=150,\n",
    "                                    max_features=13, max_leaf_nodes=50, min_samples_leaf=1,\n",
    "                                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                    presort=False, random_state=None, splitter='random')**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
